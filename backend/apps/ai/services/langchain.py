import asyncio
import os
import queue
import threading
import time
from collections.abc import Generator
from typing import Any

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

from apps.ai.services.pii_masker import PiiMasker, make_req_id, unmask_stream
from apps.ai.services.prompts import (
    ANALYSIS_SYSTEM_J2,
    ANALYSIS_USER_J2,
    BODY_SYSTEM_J2,
    BODY_USER_J2,
    INTEGRATE_SYSTEM_J2,
    INTEGRATE_USER_J2,
    PLAN_SYSTEM_J2,
    PLAN_USER_J2,
    REPLY_SYSTEM_J2,
    REPLY_USER_J2,
    SUBJECT_SYSTEM_J2,
    SUBJECT_USER_J2,
)
from apps.ai.services.utils import build_prompt_inputs, collect_prompt_context, heartbeat, sse_event

_subject_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SUBJECT_SYSTEM_J2),
        ("user", SUBJECT_USER_J2),
    ],
    template_format="jinja2",
)

_body_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", BODY_SYSTEM_J2),
        ("user", BODY_USER_J2),
    ],
    template_format="jinja2",
)

_model = ChatOpenAI(
    model=os.getenv("OPENAI_MODEL", "gpt-4.1-mini"),
    temperature=float(os.getenv("AI_TEMPERATURE", "0.4")),
)

_subject_model = ChatOpenAI(
    model=os.getenv("OPENAI_MODEL", "gpt-4.1-mini"),
    temperature=float(os.getenv("AI_SUBJECT_TEMPERATURE", "0.2")),
)

_subject_chain = _subject_prompt | _subject_model | StrOutputParser()
_body_chain = _body_prompt | _model | StrOutputParser()


def stream_mail_generation(
    user,
    subject: str | None,
    body: str | None,
    to_emails: list[str],
) -> Generator[str]:

    ctx = collect_prompt_context(user, to_emails)
    raw_inputs = build_prompt_inputs(ctx)
    raw_inputs["subject"] = subject or ""
    raw_inputs["body"] = body or ""

    # 요청별 req_id 생성 + 마스킹(제목/본문)
    req_id = make_req_id()
    masker = PiiMasker(req_id)
    masked_inputs, mapping = masker.mask_inputs(raw_inputs)

    # Ready + client-side retry hint
    yield sse_event("ready", {"ts": int(time.time() * 1000)}, retry_ms=5000)

    # 1) Subject (non-streaming) — 제목 생성
    try:
        locked_title = (_subject_chain.invoke(masked_inputs) or "").strip()
    except Exception:
        locked_title = ""

    unmasked_title = locked_title and masker and "".join(unmask_stream([locked_title], req_id, mapping)) or locked_title

    merged_subject = f"{unmasked_title}\n\n" if unmasked_title else ""
    yield sse_event("subject", {"title": unmasked_title, "text": merged_subject}, eid="0")

    seq = 1
    last_ping = time.monotonic()

    locked_inputs = {
        "locked_subject": locked_title,
        "body": masked_inputs.get("body", ""),
        "language": raw_inputs.get("language"),
        "recipients": raw_inputs.get("recipients"),
        "group_name": raw_inputs.get("group_name"),
        "group_description": raw_inputs.get("group_description"),
        "prompt_text": raw_inputs.get("prompt_text"),
        "sender_role": raw_inputs.get("sender_role"),
        "recipient_role": raw_inputs.get("recipient_role"),
        "fewshots": raw_inputs.get("fewshots", []),
    }

    try:
        raw_stream = _body_chain.stream(locked_inputs)

        for chunk in unmask_stream(raw_stream, req_id, mapping):
            if chunk:
                yield sse_event("body.delta", {"seq": seq - 1, "text": chunk}, eid=str(seq))
                seq += 1

            if time.monotonic() - last_ping > 10:
                yield heartbeat()
                last_ping = time.monotonic()

    except Exception as e:
        yield sse_event("error", {"message": str(e)}, eid=str(seq))
    finally:
        mapping.clear()
        yield sse_event("done", {"reason": "stop"}, eid=str(seq + 1))


class ReplyOption(BaseModel):
    type: str = Field(
        ...,
        description=(
            "A free-form label describing the reply type, generated by the LLM "
            "(e.g., Positive response, Scheduling adjustment, Polite apology, etc.)."
        ),
    )
    title: str = Field(..., description="Short one-line title in target language (no quotes, no trailing punctuation)")


class ReplyPlan(BaseModel):
    language: str
    options: list[ReplyOption] = Field(..., min_length=2, max_length=4)


_plan_prompt = ChatPromptTemplate.from_messages(
    [("system", PLAN_SYSTEM_J2), ("user", PLAN_USER_J2)],
    template_format="jinja2",
)

_planner_model = ChatOpenAI(
    model=os.getenv("OPENAI_MODEL", "gpt-4.1-mini"),
    temperature=float(os.getenv("AI_PLANNER_TEMPERATURE", "0.3")),
)

# LLM → Pydantic 자동 파싱
plan_chain = _plan_prompt | _planner_model.with_structured_output(ReplyPlan)

_reply_body_prompt = ChatPromptTemplate.from_messages(
    [("system", REPLY_SYSTEM_J2), ("user", REPLY_USER_J2)],
    template_format="jinja2",
)

_reply_body_model = ChatOpenAI(
    model=os.getenv("OPENAI_MODEL", "gpt-4.1-mini"),
    temperature=float(os.getenv("AI_TEMPERATURE", "0.4")),
)

reply_body_chain = _reply_body_prompt | _reply_body_model | StrOutputParser()


class SpeechAnalysis(BaseModel):
    lexical_style: str
    grammar_patterns: str
    emotional_tone: str
    figurative_usage: str
    long_sentence_ratio: str
    representative_sentences: list[str] = Field(default_factory=list)


_analysis_prompt = ChatPromptTemplate.from_messages(
    [("system", ANALYSIS_SYSTEM_J2), ("user", ANALYSIS_USER_J2)],
    template_format="jinja2",
)

_analysis_model = ChatOpenAI(
    model=os.getenv("OPENAI_MODEL", "gpt-4.1-mini"),
    temperature=float(os.getenv("AI_TEMPERATURE", "0.4")),
)

analysis_chain = _analysis_prompt | _analysis_model.with_structured_output(SpeechAnalysis)

_integrate_prompt = ChatPromptTemplate.from_messages(
    [("system", INTEGRATE_SYSTEM_J2), ("user", INTEGRATE_USER_J2)],
    template_format="jinja2",
)

_integrate_model = ChatOpenAI(
    model=os.getenv("OPENAI_MODEL", "gpt-4.1-mini"),
    temperature=float(os.getenv("AI_TEMPERATURE", "0.4")),
)

integrate_chain = _integrate_prompt | _integrate_model.with_structured_output(SpeechAnalysis)


def stream_reply_options_llm(
    *,
    user,
    subject: str | None,
    body: str | None,
    to_email: str,
) -> Generator[str]:
    ctx = collect_prompt_context(user, [to_email])
    raw = build_prompt_inputs(ctx)
    raw["incoming_subject"] = subject or ""
    raw["incoming_body"] = body or ""

    req_id = make_req_id()
    masker = PiiMasker(req_id)
    masked_inputs, mapping = masker.mask_inputs(raw)

    def unmask(chunks: list[str]) -> list[str]:
        return list(unmask_stream(chunks, req_id, mapping))

    # ready
    yield sse_event("ready", {"ts": int(time.time() * 1000)}, retry_ms=5000)

    # 옵션 설계
    plan_inputs = {
        "incoming_subject": masked_inputs["incoming_subject"],
        "incoming_body": masked_inputs["incoming_body"],
        "language": masked_inputs.get("language"),
        "recipients": masked_inputs.get("recipients"),
        "group_description": masked_inputs.get("group_description"),
        "prompt_text": masked_inputs.get("prompt_text"),
        "sender_role": masked_inputs.get("sender_role"),
        "recipient_role": masked_inputs.get("recipient_role"),
    }
    try:
        plan = plan_chain.invoke(plan_inputs)  # Pydantic ReplyPlan (v2)
    except Exception:
        # 최소 폴백 2개
        plan = type(
            "FallbackPlan",
            (object,),
            {
                "language": masked_inputs.get("language") or "user's original language",
                "options": [
                    type("Opt", (object,), {"type": "Concise reply", "title": "Quick confirmation"})(),
                    type("Opt", (object,), {"type": "Follow-up", "title": "A few clarifications"})(),
                ],
            },
        )()

    # 후처리(최대 4개 + 언마스크/트리밍)
    items = []
    for i, opt in enumerate(plan.options[:4]):
        title = "".join(unmask([opt.title])).strip().rstrip(" .」")
        otype = "".join(unmask([opt.type])).strip()
        items.append({"id": i, "type": otype, "title": title})

    # options (id=0)
    next_eid = 0
    yield sse_event("options", {"count": len(items), "items": items}, eid=str(next_eid))
    next_eid += 1

    # 옵션별 본문 스트리밍 (워커 스레드)
    q: queue.Queue[tuple[str, dict]] = queue.Queue()

    masked_common = {
        "incoming_subject": masked_inputs["incoming_subject"],
        "incoming_body": masked_inputs["incoming_body"],
        "language": plan.language,
        "recipients": masked_inputs.get("recipients"),
        "group_description": masked_inputs.get("group_description"),
        "prompt_text": masked_inputs.get("prompt_text"),
        "sender_role": masked_inputs.get("sender_role"),
        "recipient_role": masked_inputs.get("recipient_role"),
    }

    def worker(opt_idx: int, locked_type: str, locked_title: str):
        async def produce():
            seq = 0
            inputs = {
                **masked_common,
                "locked_type": locked_type,
                "locked_title": locked_title,
            }
            try:
                async for chunk in reply_body_chain.astream(inputs):
                    if not chunk:
                        continue
                    for piece in unmask([chunk]):
                        if piece:
                            q.put(("option.delta", {"id": opt_idx, "seq": seq, "text": piece}))
                            seq += 1
            except Exception as e:
                q.put(("option.error", {"id": opt_idx, "message": str(e)}))
            finally:
                q.put(("option.done", {"id": opt_idx, "total_seq": seq}))

        # 각 워커 스레드 내에서 이벤트 루프 실행
        asyncio.run(produce())

    threads = []
    for it in items:
        t = threading.Thread(
            target=worker,
            args=(it["id"], it["type"], it["title"]),
            daemon=True,
        )
        t.start()
        threads.append(t)

    last_ping = time.monotonic()
    alive = True
    while alive or not q.empty():
        try:
            event, payload = q.get(timeout=0.5)
            # 본문/완료 이벤트들: id 증가
            yield sse_event(event, payload, eid=str(next_eid))
            next_eid += 1
        except queue.Empty:
            pass

        # 10초마다 ping
        if time.monotonic() - last_ping > 10:
            yield sse_event("ping", {})
            last_ping = time.monotonic()

        alive = any(t.is_alive() for t in threads)

    mapping.clear()
    yield sse_event("done", {"reason": "all_options_finished"}, eid=str(next_eid))


def analyze_speech_llm(
    subject: str | None,
    body: str | None,
):
    analysis_input = {
        "incoming_subject": subject,
        "incoming_body": body,
    }

    analysis_result = analysis_chain.invoke(analysis_input)
    return analysis_result


def integrate_analysis(analysis_results: list[dict[str, Any]]):
    integrated_result = integrate_chain.invoke({"analysis_results": analysis_results})
    # 하나의 통합된 AnalysisResult를 반환한다.

    return integrated_result
